#!/usr/bin/env python3
# coding: utf-8

import os
import sys
import json
import copy
import asyncio
import aiohttp
import contextlib
from datetime import datetime, timedelta, timezone

# ===================== é…ç½®åŒºï¼ˆæŒ‰éœ€è°ƒæ•´ï¼‰ =====================
SEGMENTS_PER_DAY = 48                  # æ¯å¤©æ‹†æˆå‡ æ®µï¼ˆæ—¶é—´ç²’åº¦ï¼‰
MAX_CONCURRENT_ACCOUNTS = 1            # åŒæ—¶å¯åŠ¨å¤šå°‘ä¸ªè´¦æˆ·æŠ“å–ï¼ˆ1 = ä¸²è¡Œè´¦æˆ·ï¼‰
FOLLOWER_START_INTERVAL = 1            # æ¯ä¸ªä»çº¿ç¨‹å¯åŠ¨é—´éš”ï¼ˆç§’ï¼‰
FOLLOWER_RECOVERY_INTERVAL = 1         # æ¢å¤æš‚åœä»»åŠ¡æ—¶æ¯ä¸ªä»»åŠ¡çš„é—´éš”ï¼ˆç§’ï¼‰
# ============================================================

# ä»ç¯å¢ƒå˜é‡è¯»å– ACCOUNTSï¼ˆJSON å­—ç¬¦ä¸²ï¼‰
ACCOUNTS_JSON = os.getenv("ACCOUNTS_JSON")
if not ACCOUNTS_JSON:
    print("âŒ æœªæ£€æµ‹åˆ°ç¯å¢ƒå˜é‡ ACCOUNTS_JSONï¼Œè¯·åœ¨ GitHub Secrets/Variables ä¸­è®¾ç½®")
    sys.exit(1)

try:
    ACCOUNTS = json.loads(ACCOUNTS_JSON)
    if not isinstance(ACCOUNTS, dict):
        raise ValueError("ACCOUNTS_JSON must be a JSON object mapping account_id -> service_name")
except Exception as e:
    print("âŒ ACCOUNTS_JSON å†…å®¹æ— æ•ˆï¼š", e)
    sys.exit(1)

URL_TEMPLATE = "https://dash.cloudflare.com/api/v4/accounts/{account_id}/workers/observability/telemetry/query"
LOCAL_COOKIE = os.getenv("CF_COOKIE") or ""
if not LOCAL_COOKIE or len(LOCAL_COOKIE) < 20:
    print("âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆ CF_COOKIEï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡ CF_COOKIE ä¸­è®¾ç½®")
    sys.exit(1)

HEADERS = {
    "accept": "*/*",
    "content-type": "application/json",
    "origin": "https://dash.cloudflare.com",
    "referer": "https://dash.cloudflare.com/",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "workers-observability-origin": "workers-logs",
    "x-cross-site-security": "dash",
    "cookie": LOCAL_COOKIE,
}

# ===================== å·¥å…·å‡½æ•° =====================
def get_date_list(arg: str):
    """è¿”å›è¦æŸ¥è¯¢çš„æ—¥æœŸåˆ—è¡¨ï¼ˆUTCï¼Œæ ¼å¼ YYYYMMDDï¼‰"""
    if arg and arg.isdigit() and len(arg) == 8:
        return [arg]
    try:
        n = int(arg) if arg and arg.isdigit() else 7  # é»˜è®¤ 7 å¤©ï¼ˆåŒ…å«ä»Šå¤© UTCï¼‰
    except Exception:
        n = 7
    today = datetime.now(timezone.utc).date()
    return [(today - timedelta(days=i)).strftime("%Y%m%d") for i in range(n)]

def split_timeframes(date_str, segments=SEGMENTS_PER_DAY):
    """å°†ä¸€å¤©åˆ†å‰²ä¸ºè‹¥å¹²æ—¶é—´æ®µï¼ˆè¿”å› list of (start_ms,end_ms)ï¼‰"""
    dt = datetime.strptime(date_str, "%Y%m%d")
    start = datetime(dt.year, dt.month, dt.day, 0, 0, 0, tzinfo=timezone.utc)
    end = start + timedelta(days=1) - timedelta(milliseconds=1)
    start_ms = int(start.timestamp() * 1000)
    end_ms = int(end.timestamp() * 1000)
    step = (end_ms - start_ms) // segments
    arr = []
    for i in range(segments):
        s = start_ms + i * step
        e = s + step
        if i == segments - 1:
            e = end_ms
        arr.append((s, e))
    return arr

def linear_delay(attempt: int):
    """çº¿æ€§é€€é¿ï¼šç¬¬ä¸€æ¬¡ 0.5sï¼Œç¬¬äºŒæ¬¡ 1sï¼Œ... ä¸Šé™ 10s"""
    return min(0.5 * attempt, 10.0)

# ===================== æŠ“å–å®ç° =====================
async def fetch_segment_follower(session, account_id, service_name, seg_id, start_ms, end_ms,
                                 shared_progress, paused_queue: asyncio.Queue, offset=None, is_main=False, main_ok_event: asyncio.Event=None):
    """
    follower / main common worker that:
      - saves progress to shared_progress[seg_id] after each page
      - if sees shared_progress[seg_id]['take_request'] -> stop and return current progress
      - if HTTP 429 and paused_queue provided and not is_main -> enqueue paused task (seg_id,start,end,offset) and exit
      - unlimited retries with linear backoff
    Returns dict: collected logs for this segment
    """
    all_logs = {}
    page = 0

    base_data = {
        "view": "invocations",
        "queryId": "workers-logs-invocations",
        "limit": 100,
        "parameters": {
            "datasets": ["cloudflare-workers"],
            "filters": [
                {"key": "$metadata.service", "type": "string", "value": service_name, "operation": "eq"}
            ],
            "calculations": [], "groupBys": [], "havings": []
        },
        "timeframe": {"from": start_ms, "to": end_ms}
    }

    # ensure progress entry exists
    shared_progress.setdefault(seg_id, {
        "offset": offset,
        "logs": {},
        "done": False,
        "take_request": False,
        "stopped_event": asyncio.Event(),
    })

    # read initial offset if present
    cur_offset = shared_progress[seg_id].get("offset")

    while True:
        data = copy.deepcopy(base_data)
        if cur_offset:
            data["offset"] = cur_offset

        attempt = 1
        result = None
        while True:
            try:
                async with session.post(URL_TEMPLATE.format(account_id=account_id),
                                        headers=HEADERS, json=data, timeout=30) as resp:
                    status = resp.status
                    text = await resp.text()
                    if status == 200:
                        # if main was previously marked bad, mark ok now
                        if is_main and main_ok_event is not None and not main_ok_event.is_set():
                            main_ok_event.set()
                            print(f"ğŸ”” {account_id}/{service_name} ä¸»çº¿ç¨‹å·²æ¢å¤ (HTTP 200)")
                        try:
                            result = await resp.json()
                        except Exception as e:
                            print(f"âŒ {account_id}/{service_name} ç¬¬{seg_id}æ®µ JSON è§£ç å¼‚å¸¸: {e}")
                            result = None
                        # break if we got a JSON object (may be None -> will cause retry/continue)
                        if isinstance(result, dict):
                            break
                    else:
                        # non-200
                        print(f"âš ï¸ {account_id}/{service_name} ç¬¬{seg_id}æ®µ ç¬¬{page+1}é¡µ HTTP {status}")
                        if text:
                            print(f"   è¿”å›å†…å®¹: {text[:300]}")

                        # ä¸»çº¿ç¨‹é‡åˆ° 429 -> æ ‡è®° main_ok_event clear(), ä½†ä¸»çº¿ç¨‹ç»§ç»­é‡è¯•
                        if status == 429:
                            if is_main and main_ok_event is not None:
                                if main_ok_event.is_set():
                                    main_ok_event.clear()
                                    print(f"â›” {account_id}/{service_name} ä¸»çº¿ç¨‹æ£€æµ‹åˆ° 429ï¼Œè¿›å…¥é€€é¿(ä¸»çº¿ç¨‹ä»æŒç»­å°è¯•)ã€‚")
                            # å¦‚æœæ˜¯ follower (not main) and paused_queue provided -> pause this follower and return
                            if (not is_main) and paused_queue is not None:
                                print(f"â™»ï¸ {account_id}/{service_name} ç¬¬{seg_id}æ®µ ä»çº¿ç¨‹é‡åˆ° 429ï¼Œæš‚åœå¹¶å…¥é˜Ÿç­‰å¾…æ¢å¤ (offset={cur_offset})")
                                await paused_queue.put((seg_id, start_ms, end_ms, cur_offset))
                                # signal stopped
                                shared_progress[seg_id]["stopped_event"].set()
                                return all_logs
                        # otherwise will retry (linear backoff)
            except asyncio.CancelledError:
                raise
            except Exception as e:
                print(f"âŒ {account_id}/{service_name} ç¬¬{seg_id}æ®µ ç½‘ç»œ/è¯·æ±‚å¼‚å¸¸: {e}")

            # check if take_request flagged (someone requested this follower to be taken over)
            if shared_progress[seg_id].get("take_request"):
                # mark stopped and return current progress so main can take over
                print(f"ğŸ” {account_id}/{service_name} ç¬¬{seg_id} æ®µ æ”¶åˆ° take_requestï¼Œæ­£åœ¨åœæ­¢å¹¶äº¤å‡ºè¿›åº¦ (offset={cur_offset})")
                shared_progress[seg_id]["offset"] = cur_offset
                shared_progress[seg_id]["logs"] = dict(all_logs)
                shared_progress[seg_id]["stopped_event"].set()
                return all_logs

            # linear backoff, infinite retries
            delay = linear_delay(attempt)
            print(f"â³ {account_id}/{service_name} ç¬¬{seg_id} ç¬¬{page+1}é¡µ ç¬¬{attempt}æ¬¡é‡è¯•ï¼Œç­‰å¾… {delay:.1f}s")
            await asyncio.sleep(delay)
            attempt += 1

        # result obtained
        if not result or "result" not in result or "invocations" not in result["result"]:
            print(f"âŒ {account_id}/{service_name} ç¬¬{seg_id}æ®µ æ”¶åˆ°ç©ºæˆ–å¼‚å¸¸å“åº”ç»“æ„ï¼Œç»“æŸè¯¥æ®µ")
            break

        invocations = result["result"].get("invocations", {})
        if not invocations:
            # empty segment -> done
            break

        # merge into all_logs
        all_logs.update(invocations)
        page += 1
        print(f"âœ… {account_id}/{service_name} ç¬¬{seg_id}æ®µ ç¬¬{page}é¡µ è·å– {len(invocations)} æ¡æ—¥å¿—")

        # compute next offset
        next_offset = None
        for req_id in reversed(list(invocations.keys())):
            logs_list = invocations[req_id]
            if isinstance(logs_list, list) and logs_list:
                metadata = logs_list[-1].get("$metadata", {})
                next_offset = metadata.get("id")
                if next_offset:
                    break
        cur_offset = next_offset

        # save progress after each page (so takeover/resume can continue)
        shared_progress[seg_id]["offset"] = cur_offset
        # store a shallow copy of logs (to keep memory reasonable - it's user's decision)
        shared_progress[seg_id]["logs"] = dict(all_logs)

        # check if takeover requested
        if shared_progress[seg_id].get("take_request"):
            print(f"ğŸ” {account_id}/{service_name} ç¬¬{seg_id}æ®µ æ£€æµ‹åˆ° take_requestï¼Œåœæ­¢å¹¶äº¤å‡ºè¿›åº¦ (offset={cur_offset})")
            shared_progress[seg_id]["stopped_event"].set()
            return all_logs

        # continue loop; if no next_offset then segment finished
        if not cur_offset:
            break

    # mark done
    shared_progress[seg_id]["done"] = True
    shared_progress[seg_id]["stopped_event"].set()
    return all_logs

# ===================== è´¦æˆ·æŠ“å–ä¸»æµç¨‹ =====================
async def fetch_account(account_id, service_name, dates):
    """
    ä¸»æµç¨‹ï¼š
      - ä¸»çº¿ç¨‹æŠ“ç¬¬1æ®µï¼ˆæŒç»­å°è¯•ï¼‰
      - é€æ­¥å¯åŠ¨ follower æŠ“å‰©ä½™æ®µï¼ˆåŠ¨æ€å¢åŠ ï¼‰
      - follower é‡ 429 -> æ”¾å…¥ paused_queue
      - ä¸»çº¿ç¨‹å®Œæˆåæ¥ç®¡æœ€æ–° followerï¼ˆå¸¦ offset & partial logsï¼‰
      - paused_queue ä¸­çš„ä»»åŠ¡åœ¨ä¸»çº¿ç¨‹å¯ç”¨æ—¶æŒ‰é¡ºåºæ¢å¤ï¼ˆå¸¦ offsetï¼‰
    """
    async with aiohttp.ClientSession() as session:
        for date_str in dates:
            print(f"\n===== æŠ“å– {account_id}/{service_name} çš„ {date_str} æ—¥æ—¥å¿—ï¼ˆUTCï¼‰ =====")
            ranges = split_timeframes(date_str)
            all_logs = {}
            pending_segments = ranges.copy()
            paused_queue = asyncio.Queue()
            shared_progress = {}  # seg_id -> dict(progress)
            tasks = {}

            # ä¸»çº¿ç¨‹è´Ÿè´£ç¬¬1æ®µï¼ˆç¼–å· 1ï¼‰
            main_seg = pending_segments.pop(0)
            print(f"â–¶ï¸ å¯åŠ¨ä¸»çº¿ç¨‹æŠ“å–ç¬¬1æ®µ: {main_seg}")
            # main_ok_event ç”¨äºæ ‡è®°ä¸»çº¿ç¨‹å½“å‰æ˜¯å¦å¯ç”¨ï¼ˆæœªé‡ 429ï¼‰
            main_ok_event = asyncio.Event()
            main_ok_event.set()

            # start the main segment as a task but await it (main always keeps trying)
            main_task = asyncio.create_task(fetch_segment_follower(
                session, account_id, service_name, 1, main_seg[0], main_seg[1],
                shared_progress, paused_queue, offset=None, is_main=True, main_ok_event=main_ok_event
            ))
            # Wait until main_task yields first page or completes: we still await full completion later,
            # but we concurrently launch followers.
            # We'll not block here; start followers while main_task is running.
            await asyncio.sleep(0.1)

            # å¯åŠ¨ follower æŠ“å‰©ä½™æ®µï¼ˆæŒ‰é—´éš”é€æ­¥å¯åŠ¨ï¼‰
            seg_index_base = 2
            for seg_index, (s_ms, e_ms) in enumerate(pending_segments, start=seg_index_base):
                await asyncio.sleep(FOLLOWER_START_INTERVAL)
                print(f"â–¶ï¸ å¯åŠ¨ä»çº¿ç¨‹æŠ“ç¬¬{seg_index}æ®µ: {(s_ms, e_ms)}")
                # ensure progress entry exists
                shared_progress.setdefault(seg_index, {
                    "offset": None,
                    "logs": {},
                    "done": False,
                    "take_request": False,
                    "stopped_event": asyncio.Event(),
                })
                t = asyncio.create_task(fetch_segment_follower(
                    session, account_id, service_name, seg_index, s_ms, e_ms,
                    shared_progress, paused_queue, offset=None, is_main=False, main_ok_event=main_ok_event
                ))
                tasks[seg_index] = t

            # æ¢å¤å™¨ï¼šä¸»çº¿ç¨‹æ¢å¤åé€ä¸ªæ¢å¤ paused_queue
            async def recovery_loop():
                while True:
                    # when nothing to do, exit
                    if paused_queue.empty() and not tasks:
                        return
                    # only resume a paused follower when main_ok_event is set
                    if main_ok_event.is_set() and not paused_queue.empty():
                        seg_id, s_ms, e_ms, saved_offset = await paused_queue.get()
                        print(f"â™»ï¸ æ¢å¤ä»»åŠ¡: {account_id}/{service_name} ç¬¬{seg_id}æ®µ (offset={saved_offset})")
                        # ensure progress entry
                        shared_progress.setdefault(seg_id, {
                            "offset": saved_offset,
                            "logs": {},
                            "done": False,
                            "take_request": False,
                            "stopped_event": asyncio.Event(),
                        })
                        # start new follower from saved_offset
                        t = asyncio.create_task(fetch_segment_follower(
                            session, account_id, service_name, seg_id, s_ms, e_ms,
                            shared_progress, paused_queue, offset=saved_offset, is_main=False, main_ok_event=main_ok_event
                        ))
                        tasks[seg_id] = t
                        await asyncio.sleep(FOLLOWER_RECOVERY_INTERVAL)
                    else:
                        await asyncio.sleep(1)

            recovery_task = asyncio.create_task(recovery_loop())

            # ä¸»çº¿ç¨‹æ¥ç®¡é€»è¾‘ & ç»“æœåˆå¹¶å¾ªç¯
            try:
                while True:
                    # If main_task completed and no follower tasks remain and no paused tasks -> done for this date
                    if main_task.done() and not tasks and paused_queue.empty():
                        # merge main result
                        try:
                            res = main_task.result()
                            if res:
                                all_logs.update(res)
                        except Exception as e:
                            print(f"âŒ {account_id}/{service_name} ä¸»çº¿ç¨‹ ç¬¬1æ®µ å¼‚å¸¸: {e}")
                        break

                    # If main_task finished its segment early (i.e. returned), but there are follower tasks ongoing,
                    # then main should take over the latest follower task (highest seg_id)
                    if main_task.done():
                        # merge main partial result
                        try:
                            res = main_task.result()
                            if res:
                                all_logs.update(res)
                        except Exception as e:
                            print(f"âŒ {account_id}/{service_name} ä¸»çº¿ç¨‹ ç¬¬1æ®µ åˆå¹¶å¼‚å¸¸: {e}")

                        # pick latest running follower to take over
                        running_followers = [sid for sid, tk in tasks.items() if not tk.done()]
                        if running_followers:
                            latest = max(running_followers)
                            print(f"ğŸ” ä¸»çº¿ç¨‹æ¥ç®¡ä»çº¿ç¨‹ï¼š{account_id}/{service_name} ç¬¬{latest}æ®µ")
                            # request takeover
                            shared_progress.setdefault(latest, {}).setdefault("take_request", False)
                            shared_progress[latest]["take_request"] = True
                            # wait for follower to acknowledge stop (stopped_event)
                            await shared_progress[latest]["stopped_event"].wait()
                            # read progress
                            saved_offset = shared_progress[latest].get("offset")
                            partial_logs = shared_progress[latest].get("logs", {}) or {}
                            print(f"ğŸ” ä¸»çº¿ç¨‹æ¥æ‰‹ç¬¬{latest}æ®µ (offset={saved_offset})ï¼Œå·²æ”¶ {len(partial_logs)} æ¡æ—¥å¿— (from follower)")
                            # merge follower partial logs
                            all_logs.update(partial_logs)
                            # ensure follower task removed if done
                            if latest in tasks:
                                # await the task finishing (it should finish quickly because it saw take_request)
                                with contextlib.suppress(asyncio.CancelledError):
                                    await tasks[latest]
                                tasks.pop(latest, None)
                            # now main takes over remaining part of segment starting from saved_offset
                            print(f"â–¶ï¸ ä¸»çº¿ç¨‹ç»§ç»­æŠ“ç¬¬{latest}æ®µ ä» offset={saved_offset}")
                            # create a new main-style fetch for that segment (is_main=True so main_ok_event is respected)
                            main_task = asyncio.create_task(fetch_segment_follower(
                                session, account_id, service_name, latest,
                                ranges[latest - 1][0], ranges[latest - 1][1],
                                shared_progress, paused_queue, offset=saved_offset, is_main=True, main_ok_event=main_ok_event
                            ))
                            # loop continues
                            await asyncio.sleep(0.1)
                            continue
                        else:
                            # no running followers -> maybe paused or none -> if paused exists recovery loop will handle
                            await asyncio.sleep(0.5)
                            continue

                    # normal loop: collect finished follower results
                    for seg_id, t in list(tasks.items()):
                        if t.done():
                            try:
                                res = t.result()
                                if res:
                                    all_logs.update(res)
                            except Exception as e:
                                print(f"âŒ {account_id}/{service_name} ç¬¬{seg_id}æ®µ å¼‚å¸¸: {e}")
                            tasks.pop(seg_id, None)
                    await asyncio.sleep(0.5)

                # wait recovery loop to finish
                await recovery_task
            finally:
                if not recovery_task.done():
                    recovery_task.cancel()
                    with contextlib.suppress(asyncio.CancelledError):
                        await recovery_task

            # æœ€ç»ˆä¿å­˜ JSONï¼ˆæŒ‰ account+dateï¼‰
            out_file = f"{account_id}_invocations_{date_str}.json"
            with open(out_file, "w", encoding="utf-8") as f:
                json.dump({"invocations": all_logs}, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“¦ {account_id} å·²ä¿å­˜ {len(all_logs)} æ¡æ—¥å¿— -> {out_file}")

# ===================== ä¸»ç¨‹åº =====================
async def main_async():
    args = sys.argv[1:]
    # æ”¯æŒ -<account_id> é€‰æ‹©è´¦æˆ·ï¼Œä¹Ÿæ”¯æŒæ•°å­—å‚æ•°è¡¨ç¤ºå¤©æ•°æˆ– YYYYMMDD
    selected_days = None
    selected_accounts = []
    for a in args:
        if a.startswith("-"):
            selected_accounts.append(a[1:])
        elif a.isdigit():
            if len(a) == 8:
                selected_days = a  # single date
            else:
                try:
                    int(a)
                    selected_days = a
                except:
                    pass

    if selected_days is None:
        selected_days = "7"  # é»˜è®¤æœ€è¿‘7å¤©

    # Build accounts map to operate on
    if selected_accounts:
        accounts = {k: v for k, v in ACCOUNTS.items() if k in selected_accounts}
        if not accounts:
            print("âŒ æ²¡æœ‰åŒ¹é…çš„è´¦æˆ·IDï¼Œé€€å‡º")
            return
    else:
        accounts = ACCOUNTS

    # date list
    if len(selected_days) == 8 and selected_days.isdigit():
        dates = [selected_days]
    else:
        dates = get_date_list(selected_days)

    print(f"ğŸ“… æŸ¥è¯¢å¤©æ•°: {len(dates)} -> {dates}")
    print(f"ğŸ‘¥ ç›®æ ‡è´¦æˆ·: {', '.join(accounts.keys())}")

    account_list = list(accounts.items())
    # æ§åˆ¶åŒæ—¶æŸ¥è¯¢è´¦æˆ·æ•°ï¼ˆbatchï¼‰
    for i in range(0, len(account_list), MAX_CONCURRENT_ACCOUNTS):
        batch = account_list[i:i + MAX_CONCURRENT_ACCOUNTS]
        tasks = [fetch_account(acc_id, svc_name, dates) for acc_id, svc_name in batch]
        await asyncio.gather(*tasks)

if __name__ == "__main__":
    asyncio.run(main_async())
