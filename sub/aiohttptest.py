#!/usr/bin/env python3
# coding: utf-8
"""
å¼‚æ­¥æ—¥å¿—æŠ“å–è„šæœ¬ï¼ˆä¸»çº¿ç¨‹ + ä»çº¿ç¨‹æ¢å¤æœºåˆ¶ï¼‰
è¦æ±‚ï¼šPython 3.8+ï¼Œaiohttp
ç¯å¢ƒå˜é‡ï¼š
  - ACCOUNTS_JSON: JSON å­—ç¬¦ä¸²ï¼Œå½¢å¦‚ {"acctid1": "service1", ...}
  - CF_COOKIE: Cloudflare cookie å­—ç¬¦ä¸²
ç”¨æ³•ï¼š
  python sub/fetcher.py 7          # æŸ¥è¯¢æœ€è¿‘7å¤©ï¼ˆUTCï¼‰
  python sub/fetcher.py 20251101   # æŒ‡å®šæŸå¤© YYYYMMDD
  python sub/fetcher.py -68dc013... 7  # ä¹Ÿæ”¯æŒ -<account_id> é€‰æ‹©ç‰¹å®šè´¦æˆ·
"""

import os
import sys
import json
import copy
import asyncio
import aiohttp
from datetime import datetime, timedelta, timezone

# ===================== é…ç½®åŒºï¼ˆæŒ‰éœ€è°ƒæ•´ï¼‰ =====================
SEGMENTS_PER_DAY = 48                  # æ¯å¤©æ‹†æˆå‡ æ®µï¼ˆæ—¶é—´ç²’åº¦ï¼‰
MAX_CONCURRENT_ACCOUNTS = 1            # åŒæ—¶å¯åŠ¨å¤šå°‘ä¸ªè´¦æˆ·æŠ“å–ï¼ˆ1 = ä¸²è¡Œè´¦æˆ·ï¼‰
FOLLOWER_START_INTERVAL = 1            # æ¯ä¸ªä»çº¿ç¨‹å¯åŠ¨é—´éš”ï¼ˆç§’ï¼‰
FOLLOWER_RECOVERY_INTERVAL = 1         # æ¢å¤æš‚åœä»»åŠ¡æ—¶çš„é—´éš”ï¼ˆç§’ï¼‰
# ============================================================

# ä»ç¯å¢ƒå˜é‡è¯»å– ACCOUNTSï¼ˆJSON å­—ç¬¦ä¸²ï¼‰
ACCOUNTS_JSON = os.getenv("ACCOUNTS_JSON")
if not ACCOUNTS_JSON:
    print("âŒ æœªæ£€æµ‹åˆ°ç¯å¢ƒå˜é‡ ACCOUNTS_JSONï¼Œè¯·åœ¨ GitHub Secrets/Variables ä¸­è®¾ç½®")
    sys.exit(1)

try:
    ACCOUNTS = json.loads(ACCOUNTS_JSON)
    if not isinstance(ACCOUNTS, dict):
        raise ValueError("ACCOUNTS_JSON must be a JSON object")
except Exception as e:
    print("âŒ ACCOUNTS_JSON å†…å®¹æ— æ•ˆï¼š", e)
    sys.exit(1)

URL_TEMPLATE = "https://dash.cloudflare.com/api/v4/accounts/{account_id}/workers/observability/telemetry/query"
LOCAL_COOKIE = os.getenv("CF_COOKIE") or ""
if not LOCAL_COOKIE or len(LOCAL_COOKIE) < 20:
    print("âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆ CF_COOKIEï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡ CF_COOKIE ä¸­è®¾ç½®")
    sys.exit(1)

HEADERS = {
    "accept": "*/*",
    "content-type": "application/json",
    "origin": "https://dash.cloudflare.com",
    "referer": "https://dash.cloudflare.com/",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "workers-observability-origin": "workers-logs",
    "x-cross-site-security": "dash",
    "cookie": LOCAL_COOKIE,
}

# ===================== å·¥å…·å‡½æ•° =====================
def get_date_list(arg: str):
    """è¿”å›è¦æŸ¥è¯¢çš„æ—¥æœŸåˆ—è¡¨ï¼ˆUTCï¼Œæ ¼å¼ YYYYMMDDï¼‰"""
    if arg and arg.isdigit() and len(arg) == 8:
        return [arg]
    try:
        n = int(arg) if arg and arg.isdigit() else 7  # é»˜è®¤ 7 å¤©ï¼ˆåŒ…å«ä»Šå¤© UTCï¼‰
    except Exception:
        n = 7
    today = datetime.now(timezone.utc).date()
    return [(today - timedelta(days=i)).strftime("%Y%m%d") for i in range(n)]

def split_timeframes(date_str, segments=SEGMENTS_PER_DAY):
    """å°†ä¸€å¤©åˆ†å‰²ä¸ºè‹¥å¹²æ—¶é—´æ®µï¼ˆè¿”å› list of (start_ms,end_ms)ï¼‰"""
    dt = datetime.strptime(date_str, "%Y%m%d")
    start = datetime(dt.year, dt.month, dt.day, 0, 0, 0, tzinfo=timezone.utc)
    end = start + timedelta(days=1) - timedelta(milliseconds=1)
    start_ms = int(start.timestamp() * 1000)
    end_ms = int(end.timestamp() * 1000)
    step = (end_ms - start_ms) // segments
    arr = []
    for i in range(segments):
        s = start_ms + i * step
        e = s + step
        if i == segments - 1:
            e = end_ms
        arr.append((s, e))
    return arr

def linear_delay(attempt: int):
    """çº¿æ€§é€€é¿ï¼šç¬¬ä¸€æ¬¡ 0.5sï¼Œç¬¬äºŒæ¬¡ 1sï¼Œ... ä¸Šé™ 10s"""
    return min(0.5 * attempt, 10.0)

# ===================== å¼‚æ­¥æŠ“å–å‡½æ•° =====================
async def fetch_segment(session, account_id, service_name, seg_id, start_ms, end_ms, paused_queue=None, offset=None, is_main=False, main_ok_event: asyncio.Event = None):
    """
    æŠ“å–å•æ®µæ—¥å¿—ï¼ˆåˆ†é¡µ + æ— é™é‡è¯• + çº¿æ€§é€€é¿ + æ”¯æŒ offset æ¢å¤ï¼‰
    å‚æ•°:
      - paused_queue: asyncio.Queueï¼Œç”¨äºä»çº¿ç¨‹é‡åˆ° 429 æ—¶å­˜æ”¾æš‚åœä»»åŠ¡ (seg_id,start_ms,end_ms,offset)
      - offset: ç”¨äºæ¢å¤åˆ†é¡µ
      - is_main: å¦‚æœ True è¡¨ç¤ºä¸»çº¿ç¨‹ï¼ˆæ°¸è¿œæŒç»­å°è¯•ä¸”ä¼šè®¾ç½®/æ¸…é™¤ main_ok_eventï¼‰
      - main_ok_event: asyncio.Eventï¼Œä¸»çº¿ç¨‹æˆåŠŸæ—¶ set(); é‡ 429 æ—¶ clear()
    è¿”å›:
      dict æ‰€æœ‰æŠ“åˆ°çš„ invocationsï¼ˆæŒ‰åŸå§‹ API çš„ç»“æ„ï¼‰
    """
    all_logs = {}
    page = 0
    base_data = {
        "view": "invocations",
        "queryId": "workers-logs-invocations",
        "limit": 100,
        "parameters": {
            "datasets": ["cloudflare-workers"],
            "filters": [
                {"key": "$metadata.service", "type": "string", "value": service_name, "operation": "eq"}
            ],
            "calculations": [], "groupBys": [], "havings": []
        },
        "timeframe": {"from": start_ms, "to": end_ms}
    }

    # offset ç”¨äºåˆ†é¡µæ¢å¤
    while True:
        data = copy.deepcopy(base_data)
        if offset:
            data["offset"] = offset

        attempt = 1
        while True:
            try:
                async with session.post(URL_TEMPLATE.format(account_id=account_id), headers=HEADERS, json=data, timeout=30) as resp:
                    status = resp.status
                    text = await resp.text()
                    # æˆåŠŸ
                    if status == 200:
                        # å¦‚æœä¸»çº¿ç¨‹ä¹‹å‰è¢«æ ‡è®°ä¸ºä¸å¯ç”¨ï¼ˆmain_ok_event clearedï¼‰ï¼Œç°åœ¨æˆåŠŸåˆ™ set()
                        if is_main and main_ok_event is not None and not main_ok_event.is_set():
                            main_ok_event.set()
                            # ä¸»æ¢å¤ï¼šlog
                            print(f"ğŸ”” {account_id}/{service_name} ä¸»çº¿ç¨‹å·²æ¢å¤ï¼ˆHTTP 200ï¼‰")
                        # è§£æ JSON
                        try:
                            result = await resp.json()
                        except Exception as e:
                            print(f"âŒ {account_id}/{service_name} ç¬¬{seg_id}æ®µ JSON è§£ç å¼‚å¸¸: {e}")
                            # è§£æå¼‚å¸¸æŒ‰é‡è¯•å¤„ç†
                            pass
                        else:
                            break  # æˆåŠŸæ‹¿åˆ° resultï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    else:
                        # é‡åˆ°é 200
                        print(f"âš ï¸ {account_id}/{service_name} ç¬¬{seg_id}æ®µ ç¬¬{page+1}é¡µ HTTP {status}")
                        if text:
                            print(f"   è¿”å›å†…å®¹: {text[:300]}")
                        # ä¸»çº¿ç¨‹é‡åˆ° 429 -> æ ‡è®°ä¸å¯æ¢å¤çŠ¶æ€ï¼ˆclearï¼‰ï¼Œä½†æŒç»­é‡è¯•
                        if status == 429:
                            if is_main and main_ok_event is not None:
                                if main_ok_event.is_set():
                                    main_ok_event.clear()
                                    print(f"â›” {account_id}/{service_name} ä¸»çº¿ç¨‹æ£€æµ‹åˆ° 429ï¼Œåˆ‡æ¢åˆ°é€€é¿æ¨¡å¼ï¼ˆä¸»çº¿ç¨‹ä¸åœæ­¢ï¼‰")
                            # è‹¥æ˜¯ followerï¼ˆpaused_queue æä¾›ï¼‰ï¼ŒæŠŠä»»åŠ¡æŒ‚èµ·å¹¶è¿”å›
                            if paused_queue is not None and not is_main:
                                # æŠŠå½“å‰ offset ä¸€å¹¶ä¿å­˜ï¼ˆå¯èƒ½ä¸º Noneï¼‰
                                print(f"â™»ï¸ {account_id}/{service_name} ç¬¬{seg_id}æ®µ ä»çº¿ç¨‹é‡åˆ° 429ï¼Œæš‚åœå¹¶å…¥é˜Ÿç­‰å¾…æ¢å¤ (offset={offset})")
                                await paused_queue.put((seg_id, start_ms, end_ms, offset))
                                return all_logs
                            # å¦åˆ™ï¼ˆä¸»çº¿ç¨‹ï¼‰ç»§ç»­ä¸‹é¢çš„é‡è¯•é€»è¾‘
                        # å¯¹äº 5xx/4xx å…¶å®ƒç ï¼Œä¸»/ä»éƒ½å°†åœ¨ä¸‹é¢ç­‰å¾…åé‡è¯•ï¼ˆä¸»çº¿ç¨‹ä¸ä¼šåœæ­¢ï¼‰
            except asyncio.CancelledError:
                raise
            except Exception as e:
                print(f"âŒ {account_id}/{service_name} ç¬¬{seg_id}æ®µ ç½‘ç»œ/è¯·æ±‚å¼‚å¸¸: {e}")

            # çº¿æ€§é€€é¿ï¼Œæ°¸è¿œé‡è¯•ï¼ˆä¸»çº¿ç¨‹ & ä»çº¿ç¨‹çš„é429æƒ…å½¢ä¹Ÿé‡‡ç”¨æ­¤ç­–ç•¥ï¼‰
            delay = linear_delay(attempt)
            print(f"â³ {account_id}/{service_name} ç¬¬{seg_id} ç¬¬{page+1}é¡µ ç¬¬{attempt}æ¬¡é‡è¯•ï¼Œç­‰å¾… {delay:.1f}s")
            await asyncio.sleep(delay)
            attempt += 1
            # æ³¨æ„ï¼šæ— é™é‡è¯•ï¼Œä¸å†ä»¥æ¬¡æ•°ä¸ºä¸Šé™

        # åˆ°è¿™é‡Œæ‹¿åˆ° resultï¼ˆæˆ–è·³å‡ºï¼‰
        if not result or "result" not in result or "invocations" not in result["result"]:
            # éæ­£å¸¸ç»“æ„ï¼Œç»“æŸè¯¥æ®µï¼ˆé¿å…æ­»å¾ªç¯ï¼‰ï¼›ä½†å¯¹äºä¸»çº¿ç¨‹æˆ‘ä»¬ä»ç„¶ç»§ç»­å°è¯•ä¸‹ä¸€æ¬¡ï¼ˆè¿™é‡Œé€‰æ‹© break æ˜¯ä¸ºäº†å®‰å…¨ï¼‰
            print(f"âŒ {account_id}/{service_name} ç¬¬{seg_id}æ®µ æ”¶åˆ°ç©ºæˆ–å¼‚å¸¸å“åº”ç»“æ„ï¼Œç»ˆæ­¢è¯¥æ®µ")
            break

        invocations = result["result"].get("invocations", {})
        if not invocations:
            # æœ¬æ®µæ²¡æœ‰æ—¥å¿—ï¼Œç»“æŸè¯¥æ®µ
            break

        # åˆå¹¶æ—¥å¿—
        all_logs.update(invocations)
        page += 1
        print(f"âœ… {account_id}/{service_name} ç¬¬{seg_id}æ®µ ç¬¬{page}é¡µ è·å– {len(invocations)} æ¡æ—¥å¿—")

        # è®¡ç®—ä¸‹ä¸€é¡µ offsetï¼ˆåŸºäºæœ€åä¸€ä¸ª request id çš„æœ€åæ¡ç›®çš„ $metadata.idï¼‰
        offset = None
        for req_id in reversed(list(invocations.keys())):
            logs_list = invocations[req_id]
            if isinstance(logs_list, list) and logs_list:
                metadata = logs_list[-1].get("$metadata", {})
                offset = metadata.get("id")
                if offset:
                    break

        # å¦‚æœæ²¡æœ‰ offsetï¼Œè¯´æ˜å·²ç»è¯»å®Œæœ¬æ®µ
        if not offset:
            break

        # å¦åˆ™ç»§ç»­å¾ªç¯å»æŠ“ä¸‹ä¸€é¡µï¼ˆoffset å·²ç»è®¾ç½®ï¼‰
    return all_logs

# ===================== è´¦æˆ·æŠ“å–ä¸»æµç¨‹ =====================
async def fetch_account(account_id, service_name, dates):
    """
    æ¯ä¸ªè´¦æˆ·ï¼šä¸»çº¿ç¨‹æŠ“ç¬¬ä¸€æ®µå¹¶ç»´æŒ main_ok_eventï¼Œ
    ä»çº¿ç¨‹è´Ÿè´£å…¶ä½™æ®µï¼Œé‡åˆ° 429 æ”¾å…¥ paused_queueï¼Œä¸»çº¿ç¨‹æ¢å¤åé€ä¸ªæ¢å¤ paused_queue ä¸­çš„ä»»åŠ¡ï¼ˆå¸¦ offsetï¼‰
    """
    async with aiohttp.ClientSession() as session:
        for date_str in dates:
            print(f"\n===== æŠ“å– {account_id}/{service_name} çš„ {date_str} æ—¥æ—¥å¿—ï¼ˆUTCï¼‰ =====")
            ranges = split_timeframes(date_str)
            all_logs = {}
            pending_segments = ranges.copy()
            paused_queue = asyncio.Queue()
            main_ok_event = asyncio.Event()
            main_ok_event.set()  # åˆå§‹ä¸ºå¯æ¢å¤çŠ¶æ€

            # ä¸»çº¿ç¨‹è´Ÿè´£ç¬¬ 1 æ®µï¼ˆç¼–å· 1ï¼‰
            main_seg = pending_segments.pop(0)
            print(f"â–¶ï¸ å¯åŠ¨ä¸»çº¿ç¨‹æŠ“å–ç¬¬1æ®µ: {main_seg}")
            main_logs = await fetch_segment(
                session, account_id, service_name, 1, main_seg[0], main_seg[1],
                paused_queue=paused_queue, offset=None, is_main=True, main_ok_event=main_ok_event
            )
            all_logs.update(main_logs)

            # å¯åŠ¨ä»çº¿ç¨‹æŠ“å‰©ä½™æ®µï¼ˆå¹¶ä¸ä¼šé˜»å¡ä¸»çº¿ç¨‹çš„ç»§ç»­é‡è¯•â€”â€”ä¸»çº¿ç¨‹å·²å®Œæˆç¬¬ä¸€æ®µçš„æŒç»­å°è¯•ï¼‰
            tasks = {}
            for seg_index, (s_ms, e_ms) in enumerate(pending_segments, start=2):
                await asyncio.sleep(FOLLOWER_START_INTERVAL)
                print(f"â–¶ï¸ å¯åŠ¨ä»çº¿ç¨‹æŠ“ç¬¬{seg_index}æ®µ: {(s_ms, e_ms)}")
                t = asyncio.create_task(fetch_segment(
                    session, account_id, service_name, seg_index, s_ms, e_ms,
                    paused_queue=paused_queue, offset=None, is_main=False, main_ok_event=main_ok_event
                ))
                tasks[seg_index] = t

            # æ¢å¤å™¨ï¼šå½“ paused_queue æœ‰ä»»åŠ¡å¹¶ä¸”ä¸»çº¿ç¨‹å¤„äºå¯æ¢å¤çŠ¶æ€ (main_ok_event.is_set()) æ—¶
            # é€ä¸ªæ¢å¤ paused_queue ä¸­çš„ä»»åŠ¡ï¼ˆå¸¦ offsetï¼‰ï¼Œæ¢å¤é—´éš” FOLLOWER_RECOVERY_INTERVAL
            async def recovery_loop():
                while True:
                    # å¦‚æœæ²¡æœ‰ pausedã€æ²¡æœ‰æ­£åœ¨è·‘çš„ tasksï¼Œåˆ™é€€å‡º
                    if paused_queue.empty() and not tasks:
                        return
                    # åªåœ¨ä¸»çº¿ç¨‹å¯ç”¨æ—¶æ¢å¤ä¸€ä¸ª paused ä»»åŠ¡
                    if main_ok_event.is_set() and not paused_queue.empty():
                        seg_id, s_ms, e_ms, saved_offset = await paused_queue.get()
                        print(f"â™»ï¸ æ¢å¤ä»»åŠ¡: {account_id}/{service_name} ç¬¬{seg_id}æ®µ (offset={saved_offset})")
                        # å¯åŠ¨ä¸€ä¸ªæ–°ä»»åŠ¡ä» saved_offset ç»§ç»­æŠ“
                        t = asyncio.create_task(fetch_segment(
                            session, account_id, service_name, seg_id, s_ms, e_ms,
                            paused_queue=paused_queue, offset=saved_offset, is_main=False, main_ok_event=main_ok_event
                        ))
                        tasks[seg_id] = t
                        await asyncio.sleep(FOLLOWER_RECOVERY_INTERVAL)
                    else:
                        # å¦‚æœä¸»çº¿ç¨‹ä¸å¯ç”¨ æˆ– paused_queue ç©ºï¼Œç­‰ä¸€ä¼šå„¿å†æ£€æŸ¥
                        await asyncio.sleep(1)

            # ç­‰å¾… tasks å®Œæˆæˆ–åŠ å…¥æ¢å¤å¾ªç¯å¤„ç† paused_queue
            # åŒæ—¶å‘¨æœŸæ€§åˆå¹¶å·²ç»å®Œæˆçš„ä»çº¿ç¨‹ç»“æœ
            recovery_task = asyncio.create_task(recovery_loop())
            try:
                while tasks or not paused_queue.empty():
                    # æ£€æŸ¥å·²å®Œæˆä»»åŠ¡å¹¶åˆå¹¶ç»“æœ
                    for seg_id, t in list(tasks.items()):
                        if t.done():
                            try:
                                res = t.result()
                                if res:
                                    all_logs.update(res)
                            except Exception as e:
                                print(f"âŒ {account_id}/{service_name} ç¬¬{seg_id}æ®µ å¼‚å¸¸: {e}")
                            tasks.pop(seg_id)
                    await asyncio.sleep(0.5)
                # ç­‰å¾…æ¢å¤ä»»åŠ¡ç»“æŸï¼ˆresume loop é€€å‡ºï¼‰
                await recovery_task
            finally:
                # make sure recovery_task cancelled if still running
                if not recovery_task.done():
                    recovery_task.cancel()
                    with contextlib.suppress(asyncio.CancelledError):
                        await recovery_task

            # ä¿å­˜ JSONï¼ˆæŒ‰ account+dateï¼‰
            out_file = f"{account_id}_invocations_{date_str}.json"
            with open(out_file, "w", encoding="utf-8") as f:
                json.dump({"invocations": all_logs}, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“¦ {account_id} å·²ä¿å­˜ {len(all_logs)} æ¡æ—¥å¿— -> {out_file}")

# ===================== ä¸»ç¨‹åº =====================
import contextlib

async def main_async():
    args = sys.argv[1:]
    # æ”¯æŒ -<account_id> é€‰æ‹©è´¦æˆ·ï¼Œä¹Ÿæ”¯æŒæ•°å­—å‚æ•°è¡¨ç¤ºå¤©æ•°æˆ– YYYYMMDD
    selected_days = None
    selected_accounts = []
    for a in args:
        if a.startswith("-"):
            selected_accounts.append(a[1:])
        elif a.isdigit():
            if len(a) == 8:
                selected_days = a  # single date
            else:
                # numeric -> treat as number of days
                try:
                    int(a)
                    selected_days = a
                except:
                    pass

    if selected_days is None:
        selected_days = "7"  # é»˜è®¤æœ€è¿‘7å¤©

    # Build accounts map to operate on
    if selected_accounts:
        accounts = {k: v for k, v in ACCOUNTS.items() if k in selected_accounts}
        if not accounts:
            print("âŒ æ²¡æœ‰åŒ¹é…çš„è´¦æˆ·IDï¼Œé€€å‡º")
            return
    else:
        accounts = ACCOUNTS

    # date list
    if len(selected_days) == 8 and selected_days.isdigit():
        dates = [selected_days]
    else:
        dates = get_date_list(selected_days)

    print(f"ğŸ“… æŸ¥è¯¢å¤©æ•°: {len(dates)} -> {dates}")
    print(f"ğŸ‘¥ ç›®æ ‡è´¦æˆ·: {', '.join(accounts.keys())}")

    account_list = list(accounts.items())
    # æ§åˆ¶åŒæ—¶æŸ¥è¯¢è´¦æˆ·æ•°ï¼ˆbatchï¼‰
    for i in range(0, len(account_list), MAX_CONCURRENT_ACCOUNTS):
        batch = account_list[i:i + MAX_CONCURRENT_ACCOUNTS]
        tasks = [fetch_account(acc_id, svc_name, dates) for acc_id, svc_name in batch]
        await asyncio.gather(*tasks)

if __name__ == "__main__":
    asyncio.run(main_async())
