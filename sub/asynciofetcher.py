import os
import sys
import json
import copy
import asyncio
import aiohttp
from datetime import datetime, timedelta, timezone

# ===================== é…ç½®åŒº =====================
SEGMENTS_PER_DAY = 48                  # æ¯å¤©æ‹†æˆå‡ æ®µ
MAX_RETRIES = 5                        # å•é¡µè¯·æ±‚æœ€å¤§é‡è¯•æ¬¡æ•° æœªä½¿ç”¨
BACKOFF = 0                             # é‡è¯•åŸºæ•°ç§’ï¼Œ0 è¡¨ç¤ºä¸é™é€Ÿ
MAX_CONCURRENT_ACCOUNTS = 1             # åŒæ—¶æŸ¥è¯¢è´¦æˆ·æ•°
FOLLOWER_START_INTERVAL = 1             # ä»çº¿ç¨‹å¯åŠ¨é—´éš”ç§’
FOLLOWER_RECOVERY_INTERVAL = 1          # ä»çº¿ç¨‹æ¢å¤ä»»åŠ¡é—´éš”ç§’
# ==================================================

# ä»ç¯å¢ƒå˜é‡è¯»å– ACCOUNTS
ACCOUNTS_JSON = os.getenv("ACCOUNTS_JSON")
if not ACCOUNTS_JSON:
    print("âŒ æœªæ£€æµ‹åˆ°ç¯å¢ƒå˜é‡ ACCOUNTS_JSONï¼Œè¯·åœ¨ GitHub Secrets è®¾ç½®")
    sys.exit(1)

try:
    ACCOUNTS = json.loads(ACCOUNTS_JSON)
except json.JSONDecodeError:
    print("âŒ ACCOUNTS_JSON å†…å®¹ä¸æ˜¯åˆæ³• JSON")
    sys.exit(1)

URL_TEMPLATE = "https://dash.cloudflare.com/api/v4/accounts/{account_id}/workers/observability/telemetry/query"
LOCAL_COOKIE = os.getenv("CF_COOKIE") or ""
if not LOCAL_COOKIE or len(LOCAL_COOKIE) < 20:
    print("âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆ CF_COOKIEï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡ CF_COOKIE ä¸­è®¾ç½®")
    sys.exit(1)

HEADERS = {
    "accept": "*/*",
    "content-type": "application/json",
    "origin": "https://dash.cloudflare.com",
    "referer": "https://dash.cloudflare.com/",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "workers-observability-origin": "workers-logs",
    "x-cross-site-security": "dash",
    "cookie": LOCAL_COOKIE,
}


def get_date_list(arg: str):
    n = int(arg) if arg and arg.isdigit() else 1
    today = datetime.now(timezone.utc).date()
    return [(today - timedelta(days=i)).strftime("%Y%m%d") for i in range(n)]


def split_timeframes(date_str, segments=SEGMENTS_PER_DAY):
    dt = datetime.strptime(date_str, "%Y%m%d")
    start = datetime(dt.year, dt.month, dt.day, 0, 0, 0, tzinfo=timezone.utc)
    end = start + timedelta(days=1) - timedelta(milliseconds=1)
    start_ms = int(start.timestamp() * 1000)
    end_ms = int(end.timestamp() * 1000)
    step = (end_ms - start_ms) // segments
    ranges = []
    for i in range(segments):
        seg_start = start_ms + i * step
        seg_end = seg_start + step
        if i == segments - 1:
            seg_end = end_ms
        ranges.append((seg_start, seg_end))
    return ranges


async def fetch_segment(session, account_id, service_name, seg_id, start_ms, end_ms, paused_queue=None):
    """æŠ“å–å•æ®µæ—¥å¿—ï¼ˆåˆ†é¡µ + æ— é™é‡è¯• + çº¿æ€§é€€é¿ + å®‰å…¨è§£æï¼‰"""
    all_logs = {}
    offset = None
    page = 0

    base_data = {
        "view": "invocations",
        "queryId": "workers-logs-invocations",
        "limit": 100,
        "parameters": {
            "datasets": ["cloudflare-workers"],
            "filters": [
                {"key": "$metadata.service", "type": "string", "value": service_name, "operation": "eq"}
            ],
            "calculations": [],
            "groupBys": [],
            "havings": []
        },
        "timeframe": {"from": start_ms, "to": end_ms}
    }

    while True:
        data = copy.deepcopy(base_data)
        if offset:
            data["offset"] = offset

        attempt = 1
        while True:
            try:
                    async with session.post(URL_TEMPLATE.format(account_id=account_id),
                                            headers=HEADERS, json=data, timeout=15) as resp:
                        status = resp.status
                        text = await resp.text()
                        if status == 200:
                            result = await resp.json()
                            break  # æˆåŠŸé€€å‡ºé‡è¯•å¾ªç¯
                        else:
                            print(f"âš ï¸ {account_id}/{service_name} ç¬¬{seg_id}æ®µ ç¬¬{page+1}é¡µ HTTP {status}")
                            print(f"âš ï¸ è¿”å›å†…å®¹: {text[:300]}")

                            # é‡åˆ° 429 çš„ä»çº¿ç¨‹ä»»åŠ¡æš‚æ—¶æŒ‚èµ·
                            if status == 429 and paused_queue is not None:
                                print(f"â™»ï¸ {account_id}/{service_name} ç¬¬{seg_id}æ®µæš‚åœä»çº¿ç¨‹ï¼Œç­‰å¾…æ¢å¤")
                                await paused_queue.put((seg_id, start_ms, end_ms))
                                return all_logs  # æš‚åœå½“å‰æ®µï¼Œè¿”å›å·²æŠ“å–æ—¥å¿—
            except Exception as e:
                print(f"âŒ {account_id}/{service_name} ç¬¬{seg_id}æ®µ ç¬¬{page+1}é¡µ å¼‚å¸¸: {e}")

            # æ— é™é‡è¯• + çº¿æ€§é€€é¿
            delay = min(0.5 * attempt, 10)
            print(f"â³ {account_id}/{service_name} ç¬¬{seg_id}æ®µ ç¬¬{page+1}é¡µ ç¬¬ {attempt} æ¬¡é‡è¯•, ç­‰å¾… {delay:.1f}s")
            await asyncio.sleep(delay)
            attempt += 1

        # JSON å®‰å…¨è§£æ
        try:
            if not result or "result" not in result or "invocations" not in result["result"]:
                print(f"âŒ {account_id}/{service_name} ç¬¬{seg_id}æ®µ ç©ºæˆ–å¼‚å¸¸å“åº”")
                break
            invocations = result["result"].get("invocations", {})
        except Exception as e:
            print(f"âŒ {account_id}/{service_name} ç¬¬{seg_id}æ®µ JSON è§£æå¼‚å¸¸: {e}")
            await asyncio.sleep(min(0.5 * attempt, 10))
            attempt += 1
            continue  # ç»§ç»­é‡è¯•

        if not invocations:
            break

        all_logs.update(invocations)
        page += 1
        print(f"âœ… {account_id}/{service_name} ç¬¬{seg_id}æ®µ ç¬¬{page}é¡µ {len(invocations)}æ¡æ—¥å¿—")

        # è®¡ç®—ä¸‹ä¸€é¡µ offset
        offset = None
        for req_id in reversed(list(invocations.keys())):
            logs_list = invocations[req_id]
            if isinstance(logs_list, list) and logs_list:
                metadata = logs_list[-1].get("$metadata", {})
                offset = metadata.get("id")
                if offset:
                    break
        if not offset:
            break

    return all_logs



async def fetch_account(account_id, service_name, dates, sem_global: asyncio.Semaphore):
    sem_account = asyncio.Semaphore(float("inf"))
    async with aiohttp.ClientSession() as session:
        for date_str in dates:
            print(f"\n===== æŠ“å– {account_id}/{service_name} çš„ {date_str} æ—¥æ—¥å¿—ï¼ˆUTCï¼‰ =====")
            ranges = split_timeframes(date_str)
            all_logs = {}
            pending_segments = ranges.copy()
            paused_queue = asyncio.Queue()

            # ä¸»çº¿ç¨‹æŠ“ç¬¬ä¸€æ®µ
            main_seg = pending_segments.pop(0)
            main_logs = await fetch_segment(session, account_id, service_name, 1, *main_seg, paused_queue)
            all_logs.update(main_logs)

            # ä»çº¿ç¨‹æŠ“å‰©ä½™æ®µ
            tasks = {}
            for seg_id, (start_ms, end_ms) in enumerate(pending_segments, 2):
                await asyncio.sleep(FOLLOWER_START_INTERVAL)
                task = asyncio.create_task(fetch_segment(session, account_id, service_name, seg_id, start_ms, end_ms, sem_account, sem_global, paused_queue))
                tasks[seg_id] = task

            # å¾ªç¯æ¢å¤æš‚åœä»»åŠ¡
            while not paused_queue.empty() or tasks:
                # å¤„ç†å·²å®Œæˆä»»åŠ¡
                for seg_id, task in list(tasks.items()):
                    if task.done():
                        try:
                            all_logs.update(task.result())
                        except Exception as e:
                            print(f"âŒ {account_id}/{service_name} ç¬¬{seg_id}æ®µå¼‚å¸¸: {e}")
                        tasks.pop(seg_id)

                # æ¢å¤æš‚åœä»»åŠ¡
                while not paused_queue.empty():
                    seg_id, start_ms, end_ms = await paused_queue.get()
                    print(f"â™»ï¸ {account_id}/{service_name} ç¬¬{seg_id}æ®µæ¢å¤ä»»åŠ¡")
                    task = asyncio.create_task(fetch_segment(session, account_id, service_name, seg_id, start_ms, end_ms, sem_account, sem_global, paused_queue))
                    tasks[seg_id] = task
                    await asyncio.sleep(FOLLOWER_RECOVERY_INTERVAL)

                await asyncio.sleep(1)

            # ä¿å­˜ JSON
            out_file = f"{account_id}_invocations_{date_str}.json"
            with open(out_file, "w", encoding="utf-8") as f:
                json.dump({"invocations": all_logs}, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“¦ {account_id} å·²ä¿å­˜ {len(all_logs)} æ¡æ—¥å¿— -> {out_file}")


async def main_async():
    args = sys.argv[1:]
    selected_days = next((int(a) for a in args if a.isdigit()), 1)
    selected_accounts = [a[1:] for a in args if a.startswith("-")]
    if selected_accounts:
        accounts = {k: v for k, v in ACCOUNTS.items() if k in selected_accounts}
    else:
        accounts = ACCOUNTS

    print(f"ğŸ“… æŸ¥è¯¢å¤©æ•°: {selected_days}")
    print(f"ğŸ‘¥ ç›®æ ‡è´¦æˆ·: {', '.join(accounts.keys())}")
    dates = get_date_list(str(selected_days))

    sem_global = asyncio.Semaphore(float("inf"))

    # æ§åˆ¶åŒæ—¶æŸ¥è¯¢è´¦æˆ·æ•°
    account_list = list(accounts.items())
    for i in range(0, len(account_list), MAX_CONCURRENT_ACCOUNTS):
        batch = account_list[i:i + MAX_CONCURRENT_ACCOUNTS]
        tasks = [fetch_account(acc_id, svc_name, dates, sem_global) for acc_id, svc_name in batch]
        await asyncio.gather(*tasks)


if __name__ == "__main__":
    asyncio.run(main_async())


